{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from simple_io import IO\n",
    "import codecs\n",
    "from jieba import posseg\n",
    "import time\n",
    "import operator\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID = appuser_20180404153315_37775102-8b4c-40aa-91cb-61f853ad952c\n",
      "Total jobs = 2\n",
      "sql compiled.start to submit application.\n",
      "Launching Job 1 out of 2\n",
      "Running with YARN Application = application_1520329373387_280338\n",
      "Kill Command = /home/hadoop/bin/yarn application -kill application_1520329373387_280338\n",
      "\n",
      "RemoteSparkJobMonitor Query Hive on Spark job[0] stages: [0, 1, 5, 2, 6, 3, 4]\n",
      "\n",
      "Status: Running (Hive on Spark job[0])\n",
      "LocalSparkJobMonitor Job Progress Format\n",
      "CurrentTime StageId_StageAttemptId: SucceededTasksCount(+RunningTasksCount-FailedTasksCount)/TotalTasksCount\n",
      "2018-04-04 15:33:43,519\tStage-0_0: 0/3\tStage-1_0: 0/3\tStage-2_0: 0/2\tStage-3_0: 0/3\tStage-4_0: 0/3\tStage-5_0: 0/2\tStage-6_0: 0/2\t\n",
      "2018-04-04 15:33:44,527\tStage-0_0: 0(+1)/3\tStage-1_0: 0(+1)/3\tStage-2_0: 0/2\tStage-3_0: 0/3\tStage-4_0: 0/3\tStage-5_0: 0/2\tStage-6_0: 0/2\t\n",
      "2018-04-04 15:33:47,545\tStage-0_0: 0(+3)/3\tStage-1_0: 0(+1)/3\tStage-2_0: 0/2\tStage-3_0: 0/3\tStage-4_0: 0/3\tStage-5_0: 0/2\tStage-6_0: 0/2\t\n",
      "2018-04-04 15:33:49,557\tStage-0_0: 0(+3)/3\tStage-1_0: 2(+1)/3\tStage-2_0: 0/2\tStage-3_0: 0/3\tStage-4_0: 0/3\tStage-5_0: 0/2\tStage-6_0: 0/2\t\n",
      "2018-04-04 15:33:50,564\tStage-0_0: 1(+2)/3\tStage-1_0: 3/3 Finished\tStage-2_0: 0/2\tStage-3_0: 1(+2)/3\tStage-4_0: 1(+2)/3\tStage-5_0: 0/2\tStage-6_0: 0/2\t\n",
      "2018-04-04 15:33:53,577\tStage-0_0: 1(+2)/3\tStage-1_0: 3/3 Finished\tStage-2_0: 0/2\tStage-3_0: 1(+2)/3\tStage-4_0: 1(+2)/3\tStage-5_0: 0/2\tStage-6_0: 0/2\t\n",
      "2018-04-04 15:33:54,582\tStage-0_0: 1(+2)/3\tStage-1_0: 3/3 Finished\tStage-2_0: 0/2\tStage-3_0: 1(+2)/3\tStage-4_0: 3/3 Finished\tStage-5_0: 0/2\tStage-6_0: 0/2\t\n",
      "2018-04-04 15:33:55,586\tStage-0_0: 2(+1)/3\tStage-1_0: 3/3 Finished\tStage-2_0: 0/2\tStage-3_0: 2(+1)/3\tStage-4_0: 3/3 Finished\tStage-5_0: 0/2\tStage-6_0: 0/2\t\n",
      "2018-04-04 15:33:56,591\tStage-0_0: 3/3 Finished\tStage-1_0: 3/3 Finished\tStage-2_0: 0(+2)/2\tStage-3_0: 3/3 Finished\tStage-4_0: 3/3 Finished\tStage-5_0: 0(+2)/2\tStage-6_0: 0/2\t\n",
      "2018-04-04 15:33:59,607\tStage-0_0: 3/3 Finished\tStage-1_0: 3/3 Finished\tStage-2_0: 0(+2)/2\tStage-3_0: 3/3 Finished\tStage-4_0: 3/3 Finished\tStage-5_0: 0(+2)/2\tStage-6_0: 0/2\t\n",
      "2018-04-04 15:34:01,616\tStage-0_0: 3/3 Finished\tStage-1_0: 3/3 Finished\tStage-2_0: 2/2 Finished\tStage-3_0: 3/3 Finished\tStage-4_0: 3/3 Finished\tStage-5_0: 2/2 Finished\tStage-6_0: 0(+2)/2\t\n",
      "2018-04-04 15:34:02,620\tStage-0_0: 3/3 Finished\tStage-1_0: 3/3 Finished\tStage-2_0: 2/2 Finished\tStage-3_0: 3/3 Finished\tStage-4_0: 3/3 Finished\tStage-5_0: 2/2 Finished\tStage-6_0: 1(+1)/2\t\n",
      "2018-04-04 15:34:05,633\tStage-0_0: 3/3 Finished\tStage-1_0: 3/3 Finished\tStage-2_0: 2/2 Finished\tStage-3_0: 3/3 Finished\tStage-4_0: 3/3 Finished\tStage-5_0: 2/2 Finished\tStage-6_0: 1(+1)/2\t\n",
      "2018-04-04 15:34:06,636\tStage-0_0: 3/3 Finished\tStage-1_0: 3/3 Finished\tStage-2_0: 2/2 Finished\tStage-3_0: 3/3 Finished\tStage-4_0: 3/3 Finished\tStage-5_0: 2/2 Finished\tStage-6_0: 2/2 Finished\t\n",
      "Status: Finished successfully in 31.14 seconds\n",
      "Launching Job 2 out of 2\n",
      "Running with YARN Application = application_1520329373387_280338\n",
      "Kill Command = /home/hadoop/bin/yarn application -kill application_1520329373387_280338\n",
      "\n",
      "RemoteSparkJobMonitor Query Hive on Spark job[1] stages: [7]\n",
      "\n",
      "Status: Running (Hive on Spark job[1])\n",
      "LocalSparkJobMonitor Job Progress Format\n",
      "CurrentTime StageId_StageAttemptId: SucceededTasksCount(+RunningTasksCount-FailedTasksCount)/TotalTasksCount\n",
      "2018-04-04 15:34:15,795\tStage-7_0: 0(+2)/27\t\n",
      "2018-04-04 15:34:16,797\tStage-7_0: 0(+8)/27\t\n",
      "2018-04-04 15:34:19,803\tStage-7_0: 0(+12)/27\t\n",
      "2018-04-04 15:34:22,811\tStage-7_0: 0(+12)/27\t\n",
      "2018-04-04 15:34:23,815\tStage-7_0: 0(+13)/27\t\n",
      "2018-04-04 15:34:24,820\tStage-7_0: 0(+14)/27\t\n",
      "2018-04-04 15:34:25,822\tStage-7_0: 0(+15)/27\t\n",
      "2018-04-04 15:34:27,828\tStage-7_0: 0(+17)/27\t\n",
      "2018-04-04 15:34:30,834\tStage-7_0: 0(+26)/27\t\n",
      "2018-04-04 15:34:31,836\tStage-7_0: 0(+27)/27\t\n",
      "2018-04-04 15:34:32,838\tStage-7_0: 1(+26)/27\t\n",
      "2018-04-04 15:34:34,842\tStage-7_0: 2(+25)/27\t\n",
      "2018-04-04 15:34:37,848\tStage-7_0: 3(+24)/27\t\n",
      "2018-04-04 15:34:39,851\tStage-7_0: 4(+23)/27\t\n",
      "2018-04-04 15:34:40,853\tStage-7_0: 5(+22)/27\t\n",
      "2018-04-04 15:34:43,861\tStage-7_0: 6(+21)/27\t\n",
      "2018-04-04 15:34:44,864\tStage-7_0: 7(+20)/27\t\n",
      "2018-04-04 15:34:45,866\tStage-7_0: 9(+18)/27\t\n",
      "2018-04-04 15:34:48,871\tStage-7_0: 9(+18)/27\t\n",
      "2018-04-04 15:34:51,877\tStage-7_0: 10(+17)/27\t\n",
      "2018-04-04 15:34:52,879\tStage-7_0: 14(+13)/27\t\n",
      "2018-04-04 15:34:53,881\tStage-7_0: 15(+12)/27\t\n",
      "2018-04-04 15:34:56,886\tStage-7_0: 15(+12)/27\t\n",
      "2018-04-04 15:34:59,891\tStage-7_0: 15(+12)/27\t\n",
      "2018-04-04 15:35:02,897\tStage-7_0: 16(+11)/27\t\n",
      "2018-04-04 15:35:05,902\tStage-7_0: 16(+11)/27\t\n",
      "2018-04-04 15:35:08,908\tStage-7_0: 16(+11)/27\t\n",
      "2018-04-04 15:35:11,913\tStage-7_0: 16(+11)/27\t\n",
      "2018-04-04 15:35:12,914\tStage-7_0: 17(+10)/27\t\n",
      "2018-04-04 15:35:15,919\tStage-7_0: 18(+9)/27\t\n",
      "2018-04-04 15:35:18,924\tStage-7_0: 18(+9)/27\t\n",
      "2018-04-04 15:35:21,930\tStage-7_0: 18(+9)/27\t\n",
      "2018-04-04 15:35:23,933\tStage-7_0: 19(+8)/27\t\n",
      "2018-04-04 15:35:26,939\tStage-7_0: 19(+8)/27\t\n",
      "2018-04-04 15:35:29,944\tStage-7_0: 19(+8)/27\t\n",
      "2018-04-04 15:35:32,949\tStage-7_0: 19(+8)/27\t\n",
      "2018-04-04 15:35:35,955\tStage-7_0: 20(+7)/27\t\n",
      "2018-04-04 15:35:38,961\tStage-7_0: 20(+7)/27\t\n",
      "2018-04-04 15:35:41,967\tStage-7_0: 20(+7)/27\t\n",
      "2018-04-04 15:35:44,974\tStage-7_0: 20(+7)/27\t\n",
      "2018-04-04 15:35:47,979\tStage-7_0: 20(+7)/27\t\n",
      "2018-04-04 15:35:49,983\tStage-7_0: 21(+6)/27\t\n",
      "2018-04-04 15:35:52,989\tStage-7_0: 21(+6)/27\t\n",
      "2018-04-04 15:35:55,995\tStage-7_0: 21(+6)/27\t\n",
      "2018-04-04 15:35:59,001\tStage-7_0: 21(+6)/27\t\n",
      "2018-04-04 15:36:02,006\tStage-7_0: 21(+6)/27\t\n",
      "2018-04-04 15:36:05,012\tStage-7_0: 21(+6)/27\t\n",
      "2018-04-04 15:36:08,017\tStage-7_0: 22(+5)/27\t\n",
      "2018-04-04 15:36:11,022\tStage-7_0: 22(+5)/27\t\n",
      "2018-04-04 15:36:14,027\tStage-7_0: 22(+5)/27\t\n",
      "2018-04-04 15:36:17,033\tStage-7_0: 22(+5)/27\t\n",
      "2018-04-04 15:36:20,038\tStage-7_0: 22(+5)/27\t\n",
      "2018-04-04 15:36:23,044\tStage-7_0: 22(+5)/27\t\n",
      "2018-04-04 15:36:26,050\tStage-7_0: 23(+4)/27\t\n",
      "2018-04-04 15:36:29,055\tStage-7_0: 23(+4)/27\t\n",
      "2018-04-04 15:36:30,057\tStage-7_0: 24(+3)/27\t\n",
      "2018-04-04 15:36:33,063\tStage-7_0: 24(+3)/27\t\n",
      "2018-04-04 15:36:36,069\tStage-7_0: 24(+3)/27\t\n",
      "2018-04-04 15:36:39,076\tStage-7_0: 24(+3)/27\t\n",
      "2018-04-04 15:36:41,080\tStage-7_0: 25(+2)/27\t\n",
      "2018-04-04 15:36:44,085\tStage-7_0: 25(+2)/27\t\n",
      "2018-04-04 15:36:47,095\tStage-7_0: 25(+2)/27\t\n",
      "2018-04-04 15:36:50,114\tStage-7_0: 25(+2)/27\t\n",
      "2018-04-04 15:36:53,128\tStage-7_0: 25(+2)/27\t\n",
      "2018-04-04 15:36:56,135\tStage-7_0: 25(+2)/27\t\n",
      "2018-04-04 15:36:59,143\tStage-7_0: 25(+2)/27\t\n",
      "2018-04-04 15:37:02,156\tStage-7_0: 25(+2)/27\t\n",
      "2018-04-04 15:37:05,164\tStage-7_0: 25(+2)/27\t\n",
      "2018-04-04 15:37:08,183\tStage-7_0: 25(+2)/27\t\n",
      "2018-04-04 15:37:11,191\tStage-7_0: 25(+2)/27\t\n",
      "2018-04-04 15:37:14,199\tStage-7_0: 25(+2)/27\t\n",
      "2018-04-04 15:37:17,208\tStage-7_0: 25(+2)/27\t\n",
      "2018-04-04 15:37:20,222\tStage-7_0: 25(+2)/27\t\n",
      "2018-04-04 15:37:23,230\tStage-7_0: 25(+2)/27\t\n",
      "2018-04-04 15:37:26,237\tStage-7_0: 25(+2)/27\t\n",
      "2018-04-04 15:37:29,245\tStage-7_0: 25(+2)/27\t\n",
      "2018-04-04 15:37:31,251\tStage-7_0: 26(+1)/27\t\n",
      "2018-04-04 15:37:34,257\tStage-7_0: 26(+1)/27\t\n",
      "2018-04-04 15:37:37,262\tStage-7_0: 26(+1)/27\t\n",
      "2018-04-04 15:37:40,267\tStage-7_0: 26(+1)/27\t\n",
      "2018-04-04 15:37:43,274\tStage-7_0: 26(+1)/27\t\n",
      "2018-04-04 15:37:46,282\tStage-7_0: 26(+1)/27\t\n",
      "2018-04-04 15:37:49,292\tStage-7_0: 26(+1)/27\t\n",
      "2018-04-04 15:37:52,299\tStage-7_0: 26(+1)/27\t\n",
      "2018-04-04 15:37:55,313\tStage-7_0: 26(+1)/27\t\n",
      "2018-04-04 15:37:58,318\tStage-7_0: 26(+1)/27\t\n",
      "2018-04-04 15:38:00,322\tStage-7_0: 27/27 Finished\t\n",
      "Status: Finished successfully in 233.53 seconds\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "all_wxmsg=client.queryAsPandas(all_wxmsg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID = appuser_20180404155609_fa2ad8ec-1636-43db-a721-ebdc903c6f71\n",
      "Total jobs = 2\n",
      "sql compiled.start to submit application.\n",
      "Launching Job 1 out of 2\n",
      "Running with YARN Application = application_1520329373387_280338\n",
      "Kill Command = /home/hadoop/bin/yarn application -kill application_1520329373387_280338\n",
      "\n",
      "RemoteSparkJobMonitor Query Hive on Spark job[2] stages: [8]\n",
      "\n",
      "Status: Running (Hive on Spark job[2])\n",
      "LocalSparkJobMonitor Job Progress Format\n",
      "CurrentTime StageId_StageAttemptId: SucceededTasksCount(+RunningTasksCount-FailedTasksCount)/TotalTasksCount\n",
      "2018-04-04 15:56:11,468\tStage-8_0: 0/3\t\n",
      "2018-04-04 15:56:14,473\tStage-8_0: 0/3\t\n",
      "2018-04-04 15:56:17,477\tStage-8_0: 0/3\t\n",
      "2018-04-04 15:56:18,479\tStage-8_0: 0(+3)/3\t\n",
      "2018-04-04 15:56:21,485\tStage-8_0: 0(+3)/3\t\n",
      "2018-04-04 15:56:24,493\tStage-8_0: 0(+3)/3\t\n",
      "2018-04-04 15:56:25,495\tStage-8_0: 1(+2)/3\t\n",
      "2018-04-04 15:56:26,497\tStage-8_0: 3/3 Finished\t\n",
      "Status: Finished successfully in 16.03 seconds\n",
      "Launching Job 2 out of 2\n",
      "Running with YARN Application = application_1520329373387_280338\n",
      "Kill Command = /home/hadoop/bin/yarn application -kill application_1520329373387_280338\n",
      "\n",
      "RemoteSparkJobMonitor Query Hive on Spark job[3] stages: [9]\n",
      "\n",
      "Status: Running (Hive on Spark job[3])\n",
      "LocalSparkJobMonitor Job Progress Format\n",
      "CurrentTime StageId_StageAttemptId: SucceededTasksCount(+RunningTasksCount-FailedTasksCount)/TotalTasksCount\n",
      "2018-04-04 15:56:27,565\tStage-9_0: 0(+2)/14\t\n",
      "2018-04-04 15:56:30,571\tStage-9_0: 0(+4)/14\t\n",
      "2018-04-04 15:56:32,575\tStage-9_0: 0(+5)/14\t\n",
      "2018-04-04 15:56:35,580\tStage-9_0: 1(+4)/14\t\n",
      "2018-04-04 15:56:36,582\tStage-9_0: 1(+13)/14\t\n",
      "2018-04-04 15:56:39,589\tStage-9_0: 1(+13)/14\t\n",
      "2018-04-04 15:56:42,594\tStage-9_0: 1(+13)/14\t\n",
      "2018-04-04 15:56:44,602\tStage-9_0: 2(+12)/14\t\n",
      "2018-04-04 15:56:47,608\tStage-9_0: 2(+12)/14\t\n",
      "2018-04-04 15:56:48,609\tStage-9_0: 5(+9)/14\t\n",
      "2018-04-04 15:56:50,612\tStage-9_0: 6(+8)/14\t\n",
      "2018-04-04 15:56:53,617\tStage-9_0: 6(+8)/14\t\n",
      "2018-04-04 15:56:56,630\tStage-9_0: 6(+8)/14\t\n",
      "2018-04-04 15:56:59,635\tStage-9_0: 7(+7)/14\t\n",
      "2018-04-04 15:57:02,640\tStage-9_0: 8(+6)/14\t\n",
      "2018-04-04 15:57:03,642\tStage-9_0: 9(+5)/14\t\n",
      "2018-04-04 15:57:04,644\tStage-9_0: 11(+3)/14\t\n",
      "2018-04-04 15:57:06,646\tStage-9_0: 12(+2)/14\t\n",
      "2018-04-04 15:57:09,651\tStage-9_0: 12(+2)/14\t\n",
      "2018-04-04 15:57:10,654\tStage-9_0: 13(+1)/14\t\n",
      "2018-04-04 15:57:13,663\tStage-9_0: 14/14 Finished\t\n",
      "Status: Finished successfully in 47.10 seconds\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "after_mars_msg=client.queryAsPandas(after_mars_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfert_ia_customer_wechat=pd.read_csv('transfer_customer_ia.csv')\n",
    "#transfert_ia_customer_wechatid=transfert_ia_customer_wechat.as_matrix()\n",
    "#transfert_ia_customer_wechatid=tuple([tuple(row) for row in transfert_ia_customer_wechatid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of                     ia_id          customer_id          submit_time\n",
       "0     wxid_9nis050pbwcv22   wxid_1895058950615  2017-04-25 11:30:37\n",
       "1     wxid_7ldghy19v40t22            caile1985  2017-12-22 12:30:43\n",
       "2     wxid_6sm0rrow7o2222  wxid_1w8y1zow5mib21  2017-06-22 17:08:02\n",
       "3     wxid_bog38vb8wbox22  wxid_lzurod7h589411  2017-01-26 16:30:46\n",
       "4     wxid_bih2ir75304k22  wxid_q3mmarq8j94721  2018-03-29 16:26:37\n",
       "5     wxid_lqvya0d2ovgw22            sdfindnxb  2017-12-13 17:13:49\n",
       "6     wxid_ftm41p6aia4h22         huangwei5507  2017-04-10 14:36:35\n",
       "7     wxid_9aovm3o4lmqf22            nicolas72  2016-12-05 10:01:32\n",
       "8     wxid_231zv6behgxh22  wxid_ytaqam7gf45q11  2017-09-27 10:55:33\n",
       "9     wxid_bog38vb8wbox22  wxid_fok2csezvy8421  2017-05-25 10:28:00\n",
       "10    wxid_tvj3l2nky5il22       fancystar-king  2017-08-16 17:30:15\n",
       "11    wxid_bog38vb8wbox22  wxid_j2qs2b8c5eso21  2017-11-23 16:37:12\n",
       "12    wxid_dicw3d6x8s7f22   wxid_2789707896511  2017-12-14 10:19:50\n",
       "13    wxid_lqvya0d2ovgw22  wxid_ryg2awymd5mi21  2018-01-05 15:49:33\n",
       "14    wxid_6w74mtx59ath22  wxid_kbk4u72z1wvc21  2018-01-08 10:30:13\n",
       "15    wxid_01lh77bdg6hp22  wxid_pen57a751ypj21  2017-05-31 17:54:43\n",
       "16    wxid_7ldghy19v40t22           gromit-yin  2017-12-13 10:17:29\n",
       "17    wxid_yxez6pm1henb22            zonaa1979  2017-11-13 10:17:46\n",
       "18    wxid_tvj3l2nky5il22   wxid_4289052890422  2017-07-19 20:30:04\n",
       "19    wxid_j9p2ydwzqs1m22  wxid_w5q4tcipo4fg22  2018-01-26 10:30:20\n",
       "20    wxid_pv9c4s8hqami22   wxid_5869228692312  2017-08-24 18:13:33\n",
       "21    wxid_ftm41p6aia4h22         dingpeng1829  2017-04-21 10:32:12\n",
       "22    wxid_evd9byc7amc522  wxid_8k742k9cd7ne22  2018-02-14 11:33:17\n",
       "23    wxid_bh40iinrdiyu22      chenhaixiong119  2017-11-15 11:01:22\n",
       "24    wxid_pv9c4s8hqami22  wxid_94u5s25hhj3u21  2016-11-11 15:41:41\n",
       "25    wxid_ftm41p6aia4h22  wxid_1i1seevrpj2811  2017-06-19 14:30:59\n",
       "26    wxid_pbjxjteiv1tu22              wbhz-87  2017-12-13 11:08:48\n",
       "27    wxid_bog38vb8wbox22  wxid_lr88of95galj22  2017-04-08 11:43:13\n",
       "28    wxid_efdjfbdk16cf22          dawei107113  2018-04-03 14:44:19\n",
       "29    wxid_231zv6behgxh22           Ling465380  2017-09-20 10:11:18\n",
       "...                   ...                  ...                  ...\n",
       "1876  wxid_ascfyje9yjp622  wxid_buiqenlk38eu22  2017-11-08 16:08:52\n",
       "1877  wxid_86fp8xwcl5sb22  wxid_y8iix1qvckju22  2017-11-17 11:33:39\n",
       "1878  wxid_lqvya0d2ovgw22  wxid_r13guxs0ccs422  2017-08-17 17:10:16\n",
       "1879  wxid_kp2z8h54gwoa22  wxid_u5br974uefmv22  2018-03-20 17:02:12\n",
       "1880  wxid_bimlb6mew55n22  wxid_557r35wqaryd22  2018-01-05 15:01:28\n",
       "1881  wxid_dicw3d6x8s7f22  wxid_v0vku999c05k12  2018-03-07 12:56:07\n",
       "1882  wxid_1hqg6ikuev5322  wxid_jz6isv0rv4oh21  2018-03-29 17:56:29\n",
       "1883  wxid_qrtdxwww5ven22  wxid_9e88fawel1y722  2017-11-30 18:58:00\n",
       "1884  wxid_pv9c4s8hqami22  wxid_0fb721ct11n121  2018-01-04 10:52:11\n",
       "1885  wxid_54vs0yknq6q222  wxid_qs8d4uolwm7v22  2017-10-26 11:16:41\n",
       "1886  wxid_yxez6pm1henb22  wxid_tjf95i91trwy12  2017-12-19 15:19:09\n",
       "1887  wxid_pv9c4s8hqami22  wxid_hm1mkerjc3ax21  2018-02-28 16:50:56\n",
       "1888  wxid_bh40iinrdiyu22  wxid_x7d7o9mbdhbf21  2018-03-27 16:16:28\n",
       "1889  wxid_yxez6pm1henb22  wxid_o9f8y6siu8tk22  2017-11-30 15:53:31\n",
       "1890  wxid_ccmlhlwfneny22  wxid_romq434x9gbj21  2017-07-25 19:33:33\n",
       "1891  wxid_01lh77bdg6hp22  wxid_0grr0og3cue022  2017-06-23 10:21:08\n",
       "1892  wxid_4jhlc6s1xesr22  wxid_qfsjft0b08ya22  2017-11-29 10:28:47\n",
       "1893  wxid_ccmlhlwfneny22  wxid_f9oefup5q2th21  2017-09-07 17:23:23\n",
       "1894  wxid_0q7tiaoqzig222  wxid_5idjju18b0w222  2018-02-27 17:51:08\n",
       "1895  wxid_7ldghy19v40t22  wxid_x21ntqrx37bm12  2017-09-29 10:16:21\n",
       "1896  wxid_0q7tiaoqzig222  wxid_a9b3qkwzxbk321  2018-02-12 14:00:48\n",
       "1897  wxid_l26j8qsxm26g22   wxid_7479934799521  2018-01-18 10:24:33\n",
       "1898  wxid_j9p2ydwzqs1m22  wxid_75mi1zffqizw21  2017-12-26 15:12:05\n",
       "1899  wxid_yxez6pm1henb22  wxid_4uh9vgscm41q22  2017-11-01 10:16:36\n",
       "1900  wxid_ftm41p6aia4h22  wxid_rtujckj5jcx131  2017-07-03 10:17:01\n",
       "1901  wxid_0sggrg40k71g22  wxid_s5qbpne4zdil22  2018-03-09 10:33:34\n",
       "1902  wxid_54vs0yknq6q222  wxid_acn01i6xohso21  2017-12-13 13:00:34\n",
       "1903  wxid_231zv6behgxh22  wxid_2ic4ntvdwqzt22  2017-09-08 10:39:25\n",
       "1904  wxid_yxez6pm1henb22   wxid_5748317481812  2018-02-12 11:14:37\n",
       "1905  wxid_rzyutqozzsr522  wxid_vkaz2am4f5ox22  2018-03-14 10:59:12\n",
       "\n",
       "[1906 rows x 3 columns]>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transfert_ia_customer_wechat.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfert_cutomer_wechat=pd.read_csv('transfer_wechat.csv')\n",
    "transfert_cutomer_wechatid=transfert_cutomer_wechat.as_matrix().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_mars_msg=after_mars_msg.drop_duplicates(['t1.uuid'])\n",
    "#all_wxmsg=all_wxmsg.drop_duplicates(['wechat_msg.uuid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_wxmsg = all_wxmsg.rename(columns={'wechat_msg.iawechatid': 'ia_id','wechat_msg.userwechatid': 'customer_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.merge(all_wxmsg, transfert_ia_customer_wechat, on=['ia_id', 'customer_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_transfert_msg=result.groupby(['ia_id', 'customer_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1780\n"
     ]
    }
   ],
   "source": [
    "print(len(grouped_transfert_msg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distrubted(data):\n",
    "    content_time_pd=pd.DataFrame(columns = ['wechat_msg_distinct.iawechatid','wechat_msg_distinct.userwechatid',\"wechat_msg_distinct.time\"\"wechat_msg_distinct.content\", \"wechat_msg_distinct.createtime\"])\n",
    "    #first_time=[]\n",
    "    msgs_content=[]\n",
    "    ia_id=[]\n",
    "    user_id=[]\n",
    "    distribution=[]\n",
    "    time=[]\n",
    "    for groupname, groupdata in data:\n",
    "        groupdata=groupdata.sort_values(by='wechat_msg.createtime')\n",
    "        groupdata['wechat_msg.createtime']=pd.to_datetime(groupdata['wechat_msg.createtime'])\n",
    "        groupdata['submit_time']=pd.to_datetime(groupdata['submit_time'])\n",
    "        valid_groupdata=groupdata[groupdata['wechat_msg.createtime']<groupdata['submit_time']]\n",
    "        if (len(valid_groupdata)<50):\n",
    "            continue\n",
    "        #print(max(groupdata['submit_time']-groupdata['wechat_msg.createtime']).days)\n",
    "        ia_id.append(groupname[0])\n",
    "        user_id.append(groupname[1])\n",
    "        time.append(str(max(groupdata['submit_time']-groupdata['wechat_msg.createtime']).days))\n",
    "        distribution.append(len(valid_groupdata))\n",
    "        msg_content=valid_groupdata['wechat_msg.content'].as_matrix().flatten()\n",
    "        #first_chat=valid_groupdata['wechat_msg_distinct.createtime'].as_matrix().flatten()[0]\n",
    "        msgs_content.append(msg_content)\n",
    "        #first_time.append(first_chat)\n",
    "    \n",
    "    content_time_pd['wechat_msg_distinct.content']=msgs_content\n",
    "    content_time_pd['wechat_msg_distinct.time']=time\n",
    "    content_time_pd['wechat_msg_distinct.iawechatid']=ia_id\n",
    "    content_time_pd['wechat_msg_distinct.userwechatid']=user_id\n",
    "    return content_time_pd,distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_time_pd,distribution=distrubted(grouped_transfert_msg)\n",
    "#content_time_pd.to_csv(\"positive_dataframe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566\n"
     ]
    }
   ],
   "source": [
    "print(len(content_time_pd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_time_pd.to_csv(\"positive_dataframe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_time_pd=pd.read_csv(\"positive_dataframe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184.0\n",
      "266.9247005134056\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "distribution=np.array(distribution)\n",
    "median_positive=np.median(distribution)\n",
    "print(median_positive)\n",
    "print(np.average(distribution))\n",
    "print(min(distribution))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1753"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(content_time_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "untransfer_msg=after_mars_msg[~after_mars_msg['t1.userwechatid'].isin(transfert_ia_customer_wechat['customer_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "untransfer_msg=untransfer_msg.groupby(['t1.iawechatid','t1.userwechatid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def sample_selection(data):\n",
    "    #content_time_pd=pd.DataFrame(columns = ['wechat_msg_distinct.iawechatid','wechat_msg_distinct.userwechatid',\"wechat_msg_distinct.time\"\"wechat_msg_distinct.content\", \"wechat_msg_distinct.createtime\"])\n",
    "    total_msg=[]\n",
    "    labels=[]\n",
    "    now= datetime.now()\n",
    "    time=[]\n",
    "    for groupname, groupdata in data:\n",
    "        groupdata=groupdata.sort_values(by='t1.createtime')\n",
    "        groupdata['t1.createtime']=pd.to_datetime(groupdata['t1.createtime'])\n",
    "        msg_content=groupdata['t1.content'].as_matrix().flatten()\n",
    "        #print(msg_content)\n",
    "        if len(msg_content)<10:\n",
    "            continue\n",
    "        labels.append(0)\n",
    "        long_msg_content=\"\"\n",
    "        time.append(str(max(now-groupdata['t1.createtime']).days))\n",
    "        for i in range(0,len(msg_content)):\n",
    "            if i<2:\n",
    "                continue\n",
    "            long_msg_content+=str(msg_content[i])\n",
    "        ##print(long_msg_content)\n",
    "        #print(type(long_msg_content))\n",
    "        #break\n",
    "        total_msg.append(long_msg_content)\n",
    "    #total_msg=np.array(total_msg)\n",
    "    #labels=np.array(labels)\n",
    "    return total_msg,labels,time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y,time=sample_selection(untransfer_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15045\n"
     ]
    }
   ],
   "source": [
    "print(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_time_pd=content_time_pd.groupby(['wechat_msg_distinct.iawechatid','wechat_msg_distinct.userwechatid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_positive(data):\n",
    "    labels=[]\n",
    "    total_msg=[]\n",
    "    time=[]\n",
    "    for groupname, groupdata in data:\n",
    "        labels.append(1)\n",
    "        #print(type(item))\n",
    "        time.append(str(max(groupdata['wechat_msg_distinct.time'])))\n",
    "        time_content=str(groupdata['wechat_msg_distinct.content'].as_matrix().flatten())\n",
    "        total_msg.append(time_content)\n",
    "    return total_msg,labels,time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X,new_y,transfert_time=sample_positive(content_time_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1753\n",
      "1753\n",
      "1753\n"
     ]
    }
   ],
   "source": [
    "print(new_y.count(1))\n",
    "print(len(new_X))\n",
    "print(len(new_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_X=new_X+X\n",
    "sample_y=new_y+y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_time=transfert_time+time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16798\n"
     ]
    }
   ],
   "source": [
    "print(len(sample_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creadstoplist(stopwordspath):\n",
    "    stwlist = [line.strip() for line in codecs.open(stopwordspath, 'r', encoding='utf-8').readlines()]\n",
    "    tmp = range(len(stwlist))\n",
    "    return dict(zip(stwlist,tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.823 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "stwdic = creadstoplist('stopwords.txt')\n",
    "jieba.load_userdict(\"mydict.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_line(line):\n",
    "    seqlist =jieba.posseg.cut(line) # True False\n",
    "    flags=filter(lambda w: w.word not in stwdic.keys() and len(w.word.strip())>0, seqlist)\n",
    "    corpus=[w.word for w in flags]\n",
    "    str_word = ' '.join(corpus).replace(\",\",\"\")\n",
    "    str_flags = ' '.join([\"{}/{}\".format(i.word,i.flag) for i in flags])\n",
    "    #print('line length:{:d},cost time:{:f}'.format(len(corpus),time.time()-start))\n",
    "    return str_flags,str_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(line):\n",
    "    rule = re.compile(\"[^\\u4e00-\\u9fa5^.^a-z^A-Z^0-9]\")\n",
    "    line = rule.sub('',line)\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def div_word_all_words(total_content):\n",
    "    msg_feature=[]\n",
    "    for content in total_content:\n",
    "        content=remove_punctuation(content)\n",
    "        str_flags,str_word=token_line(content)\n",
    "        words=str_word.split()\n",
    "        sentence_feature=[]\n",
    "        for word in words:\n",
    "            sentence_feature.append(word)\n",
    "        msg_feature.append(sentence_feature)\n",
    "    #msg_feature=np.array(msg_feature)\n",
    "    return msg_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "div_word_X=div_word_all_words(sample_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_builder = Word2Vec.load('w2v_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_arr=[]\n",
    "for article in div_word_X:\n",
    "    length_article=len(article)\n",
    "    len_arr.append(length_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159.0\n"
     ]
    }
   ],
   "source": [
    "len_arr=np.asarray(len_arr)\n",
    "median=np.median(len_arr)\n",
    "print(median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = int(median) # 每条聊天记录最大长度 coming from k\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildWordVector(text, MAX_SEQUENCE_LENGTH,dimension, w2v_builder):\n",
    "    vec = np.zeros(dimension*MAX_SEQUENCE_LENGTH).reshape((MAX_SEQUENCE_LENGTH, dimension))\n",
    "    #count = 0.\n",
    "    i=0\n",
    "    for word in text:\n",
    "        try:\n",
    "            i+=1\n",
    "            if (i==len(vec)):\n",
    "                break\n",
    "            vec[i]=(w2v_builder[word].reshape((dimension)))\n",
    "            #count += 1.\n",
    "        except KeyError:\n",
    "            continue\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hub/lib/python3.6/site-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "numerical_train_X=[]\n",
    "for item in div_word_X:\n",
    "    vec=buildWordVector(item,MAX_SEQUENCE_LENGTH,EMBEDDING_DIM, w2v_builder)\n",
    "    numerical_train_X.append(vec)\n",
    "#numerical_train_X=np.array(numerical_train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation  import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(numerical_train_X, sample_y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train+(numerical_train_X[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=y_train+(sample_y[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.array(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=np.array(X_test)\n",
    "y_train=np.array(y_train)\n",
    "y_test=np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14438, 160, 300)\n",
      "(14438,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, Y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangshuopeng/.local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 157, 250)          225250    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 157, 250)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 52, 250)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 13000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 300)               3900300   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 301       \n",
      "=================================================================\n",
      "Total params: 4,125,851\n",
      "Trainable params: 4,125,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Input, Flatten, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=250,kernel_size=3,input_shape=(MAX_SEQUENCE_LENGTH,EMBEDDING_DIM)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(EMBEDDING_DIM, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "#plot_model(model, to_file='model.png',show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11550 samples, validate on 2888 samples\n",
      "Epoch 1/10\n",
      "11550/11550 [==============================] - 67s - loss: 2.6988 - acc: 0.8306 - val_loss: 2.5232 - val_acc: 0.8431\n",
      "Epoch 2/10\n",
      "11550/11550 [==============================] - 72s - loss: 2.7104 - acc: 0.8318 - val_loss: 2.5229 - val_acc: 0.8435\n",
      "Epoch 3/10\n",
      "11550/11550 [==============================] - 67s - loss: 2.7106 - acc: 0.8316 - val_loss: 2.5218 - val_acc: 0.8431\n",
      "Epoch 4/10\n",
      "11550/11550 [==============================] - 66s - loss: 2.7103 - acc: 0.8317 - val_loss: 2.5227 - val_acc: 0.8435\n",
      "Epoch 5/10\n",
      "11550/11550 [==============================] - 66s - loss: 2.7102 - acc: 0.8318 - val_loss: 2.5224 - val_acc: 0.8435\n",
      "Epoch 6/10\n",
      "11550/11550 [==============================] - 65s - loss: 2.7103 - acc: 0.8317 - val_loss: 2.5229 - val_acc: 0.8431\n",
      "Epoch 7/10\n",
      "11550/11550 [==============================] - 64s - loss: 2.7102 - acc: 0.8318 - val_loss: 2.5220 - val_acc: 0.8435\n",
      "Epoch 8/10\n",
      "11550/11550 [==============================] - 70s - loss: 2.7101 - acc: 0.8319 - val_loss: 2.5221 - val_acc: 0.8435\n",
      "Epoch 9/10\n",
      "11550/11550 [==============================] - 65s - loss: 2.7101 - acc: 0.8319 - val_loss: 2.5222 - val_acc: 0.8435\n",
      "Epoch 10/10\n",
      "11550/11550 [==============================] - 65s - loss: 2.7101 - acc: 0.8319 - val_loss: 2.5223 - val_acc: 0.8435\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "model.fit(X_train, y_train, validation_data=(X_val, Y_val), epochs=10, batch_size=64)\n",
    "model.save('word_vector_cnn.h')\n",
    "#print(model.evaluate(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3360/3360 [==============================] - 5s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "out = model.predict_classes(X_test)\n",
    "out = np.array(out)\n",
    "\n",
    "cohen_kappa_score(y_test,out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hub/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(input_shape=(160, 300), units=50)`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 50)                70200     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 300)               15300     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 301       \n",
      "=================================================================\n",
      "Total params: 85,801\n",
      "Trainable params: 85,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LSTM\n",
    "model_LSTM = Sequential()\n",
    "model_LSTM.add(LSTM(output_dim=50,input_shape=(MAX_SEQUENCE_LENGTH,EMBEDDING_DIM)))\n",
    "model_LSTM.add(Dropout(0.2))\n",
    "model_LSTM.add(Dense(EMBEDDING_DIM, activation='relu'))\n",
    "model_LSTM.add(Dense(1, activation='sigmoid'))\n",
    "model_LSTM.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11550 samples, validate on 2888 samples\n",
      "Epoch 1/10\n",
      "11550/11550 [==============================] - 41s - loss: 0.3726 - acc: 0.8521 - val_loss: 0.3040 - val_acc: 0.8798\n",
      "Epoch 2/10\n",
      "11550/11550 [==============================] - 39s - loss: 0.2904 - acc: 0.8887 - val_loss: 0.2800 - val_acc: 0.8961\n",
      "Epoch 3/10\n",
      "11550/11550 [==============================] - 39s - loss: 0.2622 - acc: 0.9028 - val_loss: 0.2643 - val_acc: 0.9082\n",
      "Epoch 4/10\n",
      "11550/11550 [==============================] - 40s - loss: 0.3381 - acc: 0.8516 - val_loss: 0.3121 - val_acc: 0.8722\n",
      "Epoch 5/10\n",
      "11550/11550 [==============================] - 39s - loss: 0.2773 - acc: 0.8928 - val_loss: 0.3088 - val_acc: 0.8708\n",
      "Epoch 6/10\n",
      "11550/11550 [==============================] - 39s - loss: 0.2606 - acc: 0.8952 - val_loss: 0.3061 - val_acc: 0.8722\n",
      "Epoch 7/10\n",
      "11550/11550 [==============================] - 39s - loss: 0.2312 - acc: 0.9133 - val_loss: 0.2321 - val_acc: 0.9148\n",
      "Epoch 8/10\n",
      "11550/11550 [==============================] - 39s - loss: 0.1947 - acc: 0.9303 - val_loss: 0.2050 - val_acc: 0.9262\n",
      "Epoch 9/10\n",
      "11550/11550 [==============================] - 40s - loss: 0.1757 - acc: 0.9377 - val_loss: 0.1852 - val_acc: 0.9352\n",
      "Epoch 10/10\n",
      "11550/11550 [==============================] - 39s - loss: 0.1533 - acc: 0.9458 - val_loss: 0.1700 - val_acc: 0.9432\n"
     ]
    }
   ],
   "source": [
    "model_LSTM.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "model_LSTM.fit(X_train, y_train, validation_data=(X_val, Y_val), epochs=10, batch_size=64)\n",
    "model_LSTM.save('word_vector_lstm.h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3360/3360 [==============================] - 4s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7615899599152133"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model_LSTM.predict_classes(X_test)\n",
    "out = np.array(out)\n",
    "\n",
    "cohen_kappa_score(y_test,out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 方案二，对于每个句子的特征，我们通过sentence vector的形式来表达。依然是用中位数长度来选择看的词数，但是通过累加各个特征值，并除以总得词数来表示一个句子。对于样本不均衡的问题，用smote的方法来增加正例样本数目"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from simple_io import IO\n",
    "import codecs\n",
    "from jieba import posseg\n",
    "import time\n",
    "import operator\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "w2v_builder = Word2Vec.load('w2v_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=pd.read_csv(\"corpus.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "well_words=corpus.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "well_words=well_words.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_ratio(flags):\n",
    "    #print ' '.join(sample)\n",
    "    if type(flags) is str:\n",
    "        flags = flags.split(' ')\n",
    "    else :\n",
    "        pass\n",
    "    \n",
    "    corpus = well_words\n",
    "    \n",
    "    dictionary = {}\n",
    "    count = 0\n",
    "    if flags is np.nan or flags is None:\n",
    "        return 0,-1\n",
    "    else :\n",
    "        for key in corpus:\n",
    "            dictionary[key[0]] = 0\n",
    "        for word in flags:\n",
    "            if word in dictionary.keys():\n",
    "                dictionary[word] += 1\n",
    "        for v in dictionary.values():\n",
    "            count += v\n",
    "        try :\n",
    "            ratio = float(count)/float(len(flags))\n",
    "        except ZeroDivisionError:\n",
    "            return 0,-1\n",
    "    return len(flags), ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features=pd.read_csv(\"Information/feature_content.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features=all_features.drop(['flags','flags.1','flags.2','add_time','end_time','cus_time_first','ia_time_first'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = pd.concat([all_features,all_features['content'].apply(lambda x: pd.Series(word_ratio(x)))],axis = 1)\n",
    "all_features.rename(columns={0:'text_all_seg_num',1:'high_freq_ratio'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = pd.concat([all_features,all_features['ia_content'].apply(lambda x: pd.Series(word_ratio(x)))],axis = 1)\n",
    "all_features.rename(columns={0:'ia_text_seg_num',1:'ia_high_freq_ratio'},inplace=True)\n",
    "all_features = pd.concat([all_features,all_features['cus_content'].apply(lambda x: pd.Series(word_ratio(x)))],axis = 1)\n",
    "all_features.rename(columns={0:'cus_text_seg_num',1:'cus_high_freq_ratio'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH=int((all_features['ia_text_num'].sum()+all_features['cus_text_num'].sum())/len(all_features))\n",
    "EMBEDDING_DIM=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "postive=all_features[all_features['label']==1]\n",
    "negative=all_features[all_features['label']==0]\n",
    "negative=negative[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features=pd.concat([postive,negative])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178\n"
     ]
    }
   ],
   "source": [
    "print(MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features=all_features.dropna(axis=0, how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11499"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "div_word_X=all_features['content'].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_vec(text,MAX_SEQUENCE_LENGTH,EMBEDDING_DIM, w2v_builder):\n",
    "    vec = np.zeros(EMBEDDING_DIM).reshape((1,EMBEDDING_DIM))\n",
    "    count = 0.\n",
    "    sentence=text.split(\" \")\n",
    "    for word in sentence:\n",
    "        try:\n",
    "            vec += w2v_builder[word].reshape((1,EMBEDDING_DIM))\n",
    "            count += 1.\n",
    "            if count>MAX_SEQUENCE_LENGTH:\n",
    "                break\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hub/lib/python3.6/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "setence_vec=[]\n",
    "i=0\n",
    "for item in div_word_X:\n",
    "    vec=sentence_vec(item,MAX_SEQUENCE_LENGTH,EMBEDDING_DIM, w2v_builder)\n",
    "    setence_vec.append(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "setence_vec=np.concatenate(setence_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "setence_vec=np.concatenate(numerical_train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2046822, 300)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setence_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = all_features.join(pd.DataFrame(setence_vec)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.dropna(axis=0, how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11499"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df['label'].as_matrix().flatten()\n",
    "df=df.drop(['label','content','ia_content','cus_content','ia_id','customer_wx_id'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the last item is the sentence vetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangshuopeng/.local/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation  import train_test_split\n",
    "X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "(test_vec)=X_test[:,-300:]\n",
    "train_vec=X_train[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vec=test_vec.reshape((len(X_test),EMBEDDING_DIM,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, Y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec=X_train[:,-300:]\n",
    "val_vec=X_val[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec=train_vec.reshape((len(X_train),EMBEDDING_DIM,1))\n",
    "val_vec=val_vec.reshape((len(X_val),EMBEDDING_DIM,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangshuopeng/.local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 298, 250)          1000      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 298, 250)          1000      \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 298, 250)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 74, 250)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 18500)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 300)               5550300   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 301       \n",
      "=================================================================\n",
      "Total params: 5,552,601\n",
      "Trainable params: 5,552,101\n",
      "Non-trainable params: 500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Input, Flatten, SpatialDropout1D,SpatialDropout2D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding,BatchNormalization\n",
    "from keras.models import Sequential\n",
    "setence_model = Sequential()\n",
    "setence_model.add(Conv1D(filters=250,kernel_size=3,input_shape=(EMBEDDING_DIM,1)))\n",
    "setence_model.add(BatchNormalization())\n",
    "setence_model.add(SpatialDropout1D(0.2))\n",
    "setence_model.add(MaxPooling1D(4))\n",
    "setence_model.add(Flatten())\n",
    "setence_model.add(Dense(EMBEDDING_DIM, activation='relu'))\n",
    "setence_model.add(Dense(1, activation='sigmoid'))\n",
    "setence_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7248 samples, validate on 1812 samples\n",
      "Epoch 1/5\n",
      "7248/7248 [==============================] - 39s - loss: 23.0973 - acc: 0.8413 - val_loss: 1.1524 - val_acc: 0.8560\n",
      "Epoch 2/5\n",
      "7248/7248 [==============================] - 38s - loss: 23.5723 - acc: 0.8538 - val_loss: 1.5868 - val_acc: 0.8560\n",
      "Epoch 3/5\n",
      "7248/7248 [==============================] - 38s - loss: 23.5723 - acc: 0.8538 - val_loss: 2.0942 - val_acc: 0.8560\n",
      "Epoch 4/5\n",
      "7248/7248 [==============================] - 38s - loss: 23.5723 - acc: 0.8538 - val_loss: 2.3160 - val_acc: 0.8560\n",
      "Epoch 5/5\n",
      "7248/7248 [==============================] - 42s - loss: 23.5723 - acc: 0.8538 - val_loss: 2.3216 - val_acc: 0.8560\n"
     ]
    }
   ],
   "source": [
    "class_weight = {0 : 1.,1: 10.,}\n",
    "setence_model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "setence_model.fit(train_vec, y_train, validation_data=(val_vec, Y_val), epochs=5, batch_size=128,shuffle=True, class_weight = class_weight)\n",
    "setence_model.save('sentence_vector_cnn.h')\n",
    "#print(setence_model.evaluate(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7248/7248 [==============================] - 11s    \n",
      "[[9.8332827e-14]\n",
      " [1.7929767e-12]\n",
      " [1.5080224e-11]\n",
      " ...\n",
      " [3.5911416e-13]\n",
      " [3.7783166e-12]\n",
      " [6.9649770e-09]]\n"
     ]
    }
   ],
   "source": [
    "new_train_feature=setence_model.predict_proba(train_vec)\n",
    "print(new_train_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2240/2265 [============================>.] - ETA: 0s[[1.0569648e-14]\n",
      " [2.5920806e-12]\n",
      " [1.6774478e-14]\n",
      " ...\n",
      " [3.0299840e-10]\n",
      " [1.6315036e-11]\n",
      " [2.2406033e-12]]\n"
     ]
    }
   ],
   "source": [
    "new_test_feature=setence_model.predict_proba(test_vec)\n",
    "print(new_test_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec=train_vec.reshape((len(X_train),EMBEDDING_DIM))\n",
    "test_vec=test_vec.reshape((len(X_test),EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc=RandomForestClassifier()\n",
    "rfc_model=rfc.fit(train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_train_feature=rfc_model.predict_proba(train_vec)[:,1]\n",
    "rfc_test_feature=rfc_model.predict_proba(test_vec)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rfc_train=np.column_stack((X_train_nf,rfc_train_feature))\n",
    "X_rfc_test=np.column_stack((X_test_nf,rfc_test_feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangshuopeng/.local/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6888888888888889"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_gbm = xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05).fit(X_rfc_train, y_train)\n",
    "rfc_proba = rfc_gbm.predict_proba(X_rfc_test)[:,1]\n",
    "y_rfc_pre=rfc_gbm.predict(X_rfc_test)\n",
    "up =  heapq.nlargest(int(0.2*len(rfc_proba)), range(len(rfc_proba)), rfc_proba.__getitem__)\n",
    "recall_score(y_test[up], y_rfc_pre[up])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC(probability=True)\n",
    "svm_model = clf.fit(train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_feature=svm_model.predict_proba(train_vec)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test_feature=svm_model.predict_proba(test_vec)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_nf=X_train[:,:27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.column_stack((X_train_nf,new_train_feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_nf=X_test[:,:27]\n",
    "X_test=np.column_stack((X_test_nf,new_test_feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "gbm = xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05).fit(X_train, y_train)\n",
    "predictions = gbm.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_proba=predictions[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangshuopeng/.local/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "y_pre=gbm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexs =  heapq.nlargest(int(0.2*len(predict_proba)), range(len(predict_proba)), predict_proba.__getitem__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7218045112781954"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "recall_score(y_test[indexs], y_pre[indexs])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_1 = xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05).fit(X_train_nf, y_train)\n",
    "without_pro = gbm_1.predict_proba(X_test_nf)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangshuopeng/.local/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "without_y_pre=gbm_1.predict(X_test_nf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "top =  heapq.nlargest(int(0.2*len(without_pro)), range(len(without_pro)), without_pro.__getitem__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.673469387755102"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test[top], without_y_pre[top])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
